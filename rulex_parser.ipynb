{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pycodestyle_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from feature import Feature, Level\n",
    "from rule import Rule\n",
    "import random  # This is used to generate random level names.\n",
    "random.seed(10)\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_levels(text):\n",
    "    # print(\"Value at ({}, {}):\".format(index, column), df.loc[index, column])\n",
    "    # 'feature in {level, level, level, ...}'\n",
    "    pattern = r\"([^\\*]+) in \\{([^\\}]+)\\}\"\n",
    "\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    if matches:\n",
    "        feature = matches[0][0].strip(\"' \")\n",
    "        levels = [level.strip('\" ') for level in matches[0][1].split(',')]\n",
    "        #   print(\"Feature:\", feature)\n",
    "        #   print(\"Levels:\", levels)\n",
    "    else:\n",
    "        return False, False\n",
    "\n",
    "    return feature, levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_min_max_range(text, df):\n",
    "\n",
    "    EPSILON = 0.0001\n",
    "\n",
    "    # Inequality like: '(value (<=, <)) feature ((<=, <) value)'\n",
    "    # Left and right size are optional, but it must have at least one.\n",
    "    # Operators can be < or <=\n",
    "    pattern = r'(-?\\d+)?\\s*([<>]=?)?\\s*((?:\"[\\w\\s-]+\")|(?:\\'[\\w\\s-]+\\'|\\w+(?:-?\\w+)*))\\s*([<>]=?)?\\s*(-?\\d+)?'\n",
    "\n",
    "    match = re.match(pattern, text)\n",
    "\n",
    "    if not match:\n",
    "        return False, False, False\n",
    "\n",
    "    number1 = float(match.group(1)) if match.group(1) else None\n",
    "    op1 = match.group(2) if match.group(2) else None\n",
    "    feature = match.group(3).strip(\"\\'\")\n",
    "    op2 = match.group(4) if match.group(4) else None\n",
    "    number2 = float(match.group(5)) if match.group(5) else None\n",
    "\n",
    "    if number1 is None and number2 is None:\n",
    "        print (\"Range patter could not identify any limits\")\n",
    "        raise TypeError\n",
    "\n",
    "\n",
    "    # Limited on the left and right\n",
    "    # This case can only have signs < and <=\n",
    "    if number1 is not None and number2 is not None:\n",
    "\n",
    "        min = number1\n",
    "        max = number2\n",
    "\n",
    "        if \"=\" not in op1:\n",
    "            min += EPSILON\n",
    "        \n",
    "        if \"=\" not in op2:\n",
    "            max -= EPSILON\n",
    "\n",
    "    # Only limited on the right\n",
    "    if number1 is None and op1 is None:\n",
    "        if \">\" in op2:\n",
    "            max = df[feature].max()\n",
    "            min = number2\n",
    "            if \"=\" not in op2:\n",
    "                min += EPSILON\n",
    "            \n",
    "        \n",
    "        if \"<\" in op2:\n",
    "            min = df[feature].min()\n",
    "            max = number2\n",
    "            if \"=\" not in op2:\n",
    "                max -= EPSILON\n",
    "    \n",
    "    # Only limited on the left\n",
    "    if number2 is None and op2 is None:\n",
    "        if \">\" in op1:\n",
    "            min = df[feature].min()\n",
    "            max = number1\n",
    "            if \"=\" not in op1:\n",
    "                max -= EPSILON\n",
    "              \n",
    "        if \"<\" in op1:\n",
    "            max = df[feature].max()\n",
    "            min = number1\n",
    "            if \"=\" not in op1:\n",
    "                min += EPSILON\n",
    "\n",
    "    # print(\"First number:\", number1)\n",
    "    # print(\"Operator 1:\", op1)\n",
    "    # print(\"Word:\", feature)\n",
    "    # print(\"Operator 2:\", op2)\n",
    "    # print(\"Second number:\", number2)\n",
    "    \n",
    "    return feature, min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rules_and_update_feature_set(rules_df: pd.core.frame.DataFrame,  # DF with rules from Rulex\n",
    "                                        df: pd.core.frame.DataFrame,  # DF with data used to built rules\n",
    "                                        # Conclusion set as a list of tuples [(category, min max), ...]\n",
    "                                        conclusion_set: list,\n",
    "                                        categories_set: set,  # Set of possible category values to be employed\n",
    "                                        # Current list with all features\n",
    "                                        feature_set: list[Feature] = None) -> tuple:\n",
    "    # and levels that will be updated\n",
    "    \"\"\" Dataframe for rules needs a structure like this,\n",
    "            ID  Score Normalized   Condition 1      Condition 2             Condition 3            \n",
    "        0\t1\t0.25               0 < age <= 35\t'education-num' <= 12\t'capital-gain' <= 5119\n",
    "\n",
    "        Condition 4 \n",
    "        relationship in {\"Not-in-family\", \"Other-relative\", \"Own-child\", Unmarried}\n",
    "\n",
    "        Hence, the conditions for each rule can be an inequalitity or set of strings.\n",
    "\n",
    "        The original dataframe (df) is necessary to get min and max for unbounded ranges.\n",
    "\n",
    "        This Function will update the feature set list like: [Feature, Feature, Feature, ...]\n",
    "        Each instance of Feature contains a list of levels each of type Level\n",
    "\n",
    "        This function also returns a list of Rules extracted from the rules_df.\n",
    "        This list can later be imported by the argumentation framework.\n",
    "    \"\"\"\n",
    "    def append_feature(feature_list, feature) -> None:\n",
    "        for f in feature_list:  # Check if feature is already in the list\n",
    "            if f.name == feature:\n",
    "                break\n",
    "        else:\n",
    "            feature_list.append(Feature(feature))\n",
    "\n",
    "    if feature_set is None:\n",
    "        feature_set = []\n",
    "\n",
    "    # List of Rules to be created\n",
    "    rules = []  \n",
    "\n",
    "    for index in rules_df.index:\n",
    "        # Create current rule\n",
    "        rule = Rule(index)\n",
    "\n",
    "        for column in rules_df.columns:\n",
    "\n",
    "            if \"Score Normalized\" in column:\n",
    "                rule.weight = rules_df.loc[index, column]\n",
    "\n",
    "            if \"Condition\" not in column:\n",
    "                continue\n",
    "\n",
    "            if type(rules_df.loc[index, column]) != str:\n",
    "                continue\n",
    "\n",
    "            # Get feature and levels if condition is categorical\n",
    "            feature, levels = extract_feature_levels(\n",
    "                rules_df.loc[index, column])\n",
    "\n",
    "            if feature:\n",
    "\n",
    "                append_feature(feature_set, feature)\n",
    "\n",
    "                # Need to find feature in the list\n",
    "                for i, f in enumerate(feature_set):\n",
    "\n",
    "                    if f.name != feature:\n",
    "                        continue\n",
    "\n",
    "                    for level in levels:\n",
    "\n",
    "                        # Debug condition and extracted feature level\n",
    "                        # print(rules_df.loc[index, column])\n",
    "\n",
    "                        try:\n",
    "                            # If the category is a number, there is no need to assign a counter.\n",
    "                            new_level = Level(\n",
    "                                feature, \"category\", level, float(level), float(level))\n",
    "                            # Level value employed is discarded from possible values to be used\n",
    "                            categories_set.discard(float(level))\n",
    "                            # Debug condition and extracted feature level\n",
    "                            # print(feature, level, float(level), float(level))\n",
    "                        except ValueError:\n",
    "                            # Get a possible value from the set of values not used to assign to this category\n",
    "                            value = next(iter(categories_set))\n",
    "                            new_level = Level(\n",
    "                                feature, \"category\", level, value, value)\n",
    "                            # Level value employed is discarded from possible values to be used\n",
    "                            categories_set.discard(value)\n",
    "                            # Debug condition and extracted feature level\n",
    "                            # print(feature, level, len(feature_set[i]) + 1, len(feature_set[i]) + 1)\n",
    "\n",
    "                        # Try to add in case there is not another level with the same range\n",
    "                        feature_set[i].add_level(new_level)\n",
    "                        #rule_conditions.append(copy.deepcopy(new_level))\n",
    "                        rule.conditions.append(copy.deepcopy(new_level))\n",
    "\n",
    "                    break  # Feature found in the list\n",
    "\n",
    "                continue  # Got to next condition\n",
    "\n",
    "            # Get min and max range if condition is categorical\n",
    "            feature, min, max = extract_min_max_range(\n",
    "                rules_df.loc[index, column], df)\n",
    "\n",
    "            if feature:\n",
    "\n",
    "                # Check if feature is already in the list\n",
    "                append_feature(feature_set, feature)\n",
    "\n",
    "                # Need to find feature in the list\n",
    "                for i, f in enumerate(feature_set):\n",
    "\n",
    "                    if f.name != feature:\n",
    "                        continue\n",
    "\n",
    "                    # Debug condition and extracted feature level\n",
    "                    # print(rules_df.loc[index, column])\n",
    "                    # print(feature, min, max)\n",
    "\n",
    "                    new_level = Level(feature, \"range\",\n",
    "                                      Feature.get_new_level(), min, max)\n",
    "                    feature_set[i].add_level(new_level)\n",
    "\n",
    "                    # Get level is necessary in case we try to add a level that was already there\n",
    "                    # rule_conditions.append(copy.deepcopy(\n",
    "                    #     feature_set[i].get_level(min, max)))\n",
    "                    rule.conditions.append(copy.deepcopy(\n",
    "                        feature_set[i].get_level(min, max)))\n",
    "\n",
    "                continue  # Got to next condition\n",
    "\n",
    "            print(\"Condition does not match any pattern\")\n",
    "            print(rules_df.loc[index, column])\n",
    "            raise TypeError\n",
    "\n",
    "        rule.rule_str = rule.get_rule_str(rules_df, conclusion_set)\n",
    "        rules.append(rule)\n",
    "\n",
    "    return feature_set, rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_set_to_json(feature_set: list[Feature],\n",
    "                        conclusion_set: list[tuple],\n",
    "                        name: str = \"featureset\") -> str:\n",
    "    \"\"\" Get feature_set from create_feature_set and a conclusion set in a list of tuples.\n",
    "        Returns the json string to be imported as the feature_set\n",
    "        Example of json structure being employed: https://lucasrizzo.com/framework/json_example.png\n",
    "    \"\"\"\n",
    "\n",
    "    json_str = f\"{{\\\"featureset\\\":\\\"{name}\\\",\"\n",
    "    json_str += f\"\\n\\t\\\"attributes\\\":[\"\n",
    "\n",
    "    for att in feature_set:\n",
    "        json_str += f\"\\n\\t\\t[{{\\\"name\\\":\\\"{att.name}\\\"}},\"\n",
    "        json_str += f\"\\n\\t\\t {{\\\"range\\\":\\\"{len(att)}\\\"}},\"\n",
    "\n",
    "        # From loop\n",
    "        json_str += f\"\\n\\t\\t {{\\\"from\\\":[\"\n",
    "        for level in att.levels:\n",
    "            json_str += f\"\\n\\t\\t  \\t{{\\\"value\\\":\\\"{level.min:.4f}\\\"}},\"\n",
    "\n",
    "        json_str = json_str[:-1]  # Remove last coma\n",
    "        json_str += \"]},\"\n",
    "\n",
    "        # To loop\n",
    "        json_str += f\"\\n\\t\\t {{\\\"to\\\":[\"\n",
    "        # for level in feature_set[att]:\n",
    "        for level in att.levels:\n",
    "            json_str += f\"\\n\\t\\t  \\t{{\\\"value\\\":\\\"{level.max:.4f}\\\"}},\"\n",
    "\n",
    "        json_str = json_str[:-1]  # Remove last coma\n",
    "        json_str += \"]},\"\n",
    "\n",
    "        # Level loop\n",
    "        json_str += f\"\\n\\t\\t {{\\\"level\\\":[\"\n",
    "        for level in att.levels:\n",
    "            json_str += f\"\\n\\t\\t  \\t{{\\\"value\\\":\\\"{level.name}\\\"}},\"\n",
    "\n",
    "        json_str = json_str[:-1]  # Remove last coma\n",
    "        json_str += \"]}],\"\n",
    "\n",
    "    json_str = json_str[:-1]  # Remove last coma\n",
    "    json_str += \"\\n\\t],\"\n",
    "\n",
    "    json_str += f\"\\n\\t\\\"conclusions\\\":[\"\n",
    "\n",
    "    # for key, value in conclusion_set.items():\n",
    "    for name, min, max in conclusion_set:\n",
    "        json_str += f\"\\n\\t\\t[{{\\\"category\\\":\\\"{name}\\\"}},\"\n",
    "        json_str += f\"\\n\\t\\t {{\\\"from\\\":\\\"{min}\\\"}},\"\n",
    "        json_str += f\"\\n\\t\\t {{\\\"to\\\":\\\"{max}\\\"}}],\"\n",
    "\n",
    "    json_str = json_str[:-1] + \"]\"  # Remove last coma\n",
    "    json_str += \"\\n}\"\n",
    "\n",
    "    return json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules_to_json(rules: list[Rule]) -> None:\n",
    "\n",
    "    json_result = '{\"nodes\":['\n",
    "    node_id = 0\n",
    "    x = 0\n",
    "    y = 0\n",
    "\n",
    "    for rule in rules:\n",
    "        json_result += f'{{\\\"id\\\":{node_id},'\n",
    "        json_result += f'\\\"title\\\":\\\"R{node_id + 1}\\\",'\n",
    "        json_result += f'\\\"x\\\":{x},'\n",
    "        json_result += f'\\\"y\\\":{y},'\n",
    "        json_result += f'\\\"weight\\\":\"{rule.weight}\",'\n",
    "        json_result += f'\\\"tooltip\\\":\\\"{\n",
    "            rule.rule_str.replace('\"', '\\\\\"').strip(\"\\n\")}\\\"}},'\n",
    "        x += 150\n",
    "        node_id += 1\n",
    "        if x == 1050:  # After 6 nodes, restart row\n",
    "            x = 0\n",
    "            y += 150\n",
    "\n",
    "    json_result = json_result[:-1] + \"],\\\"edges\\\":[]}\"\n",
    "    return json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rulex(root_folder: str,  # root folder where files are being stored\n",
    "                df: pd.core.frame.DataFrame,  # data frame used to generate the rules\n",
    "                file_rules_name: str,  # xlsx file with all the rules generated by rules\n",
    "                conclusion_set: list[tuple],  # Conclusion set of rules\n",
    "                data_name: str,  # Name used to the feature set\n",
    "                max_categories: int = 1000000) -> None:\n",
    "\n",
    "    random_values = set()\n",
    "    while len(random_values) < max_categories:\n",
    "        random_values.add(random.randint(0, max_categories * 10))\n",
    "\n",
    "    xls = pd.ExcelFile(file_rules_name)\n",
    "\n",
    "    feature_set = []\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        rules_df = pd.read_excel(file_rules_name, sheet_name=sheet_name)\n",
    "\n",
    "        file_rule_name = \"\".join(\n",
    "            x for x in sheet_name if x.isalnum()) + \"_rules\"\n",
    "        rule_file_txt = root_folder + \"results framework/\" + file_rule_name + \".txt\"\n",
    "        rule_file_json = root_folder + \"results framework/\" + file_rule_name + \".json\"\n",
    "\n",
    "        feature_set, rules = create_rules_and_update_feature_set(\n",
    "            rules_df, df, conclusion_set, random_values, feature_set)\n",
    "\n",
    "        file = open(rule_file_txt, \"w\")\n",
    "        for rule in rules:\n",
    "            file.write(str(rule))\n",
    "            file.write(\"\\n\")\n",
    "        file.close()\n",
    "\n",
    "        file = open(rule_file_json, \"w\")\n",
    "        file.write(rules_to_json(rules))\n",
    "        file.close()\n",
    "\n",
    "    json_str = \"\"\n",
    "    json_str += feature_set_to_json(feature_set, conclusion_set, data_name)\n",
    "    file = open(root_folder + \"results framework/\" + data_name.capitalize() + \"_featureset.json\", \"w\")\n",
    "    file.write(json_str)\n",
    "    file.close()\n",
    "    # print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not Rule",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m file_rules_name \u001b[38;5;241m=\u001b[39m root_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Scoring_Rules_Adult.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m conclusion_set \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m----> 7\u001b[0m \u001b[43mparse_rulex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbank_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_rules_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconclusion_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madult\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m, in \u001b[0;36mparse_rulex\u001b[1;34m(root_folder, df, file_rules_name, conclusion_set, data_name, max_categories)\u001b[0m\n\u001b[0;32m     26\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(rule_file_txt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rule \u001b[38;5;129;01min\u001b[39;00m rules:\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m file\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mTypeError\u001b[0m: write() argument must be str, not Rule"
     ]
    }
   ],
   "source": [
    "# Adult file configuration\n",
    "root_folder = \"./data/Adult/unlimited_conditions/\"\n",
    "bank_df = pd.read_csv(\"./data/Adult/adult_numeric.csv\")\n",
    "file_rules_name = root_folder + '/Scoring_Rules_Adult.xlsx'\n",
    "conclusion_set = [(\"no\", 0, 0), (\"yes\", 1, 1)]\n",
    "\n",
    "parse_rulex(root_folder, bank_df, file_rules_name, conclusion_set, \"adult\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bank file configuration\n",
    "# root_folder = \"./data/Bank/4_conditions/\"\n",
    "# bank_df = pd.read_csv(\"./data/Bank/bank_numeric.csv\")\n",
    "# file_rules_name = root_folder + '/Scoring_Rules_Bank.xlsx'\n",
    "# conclusion_set = [(\"no\", 0, 0), (\"yes\", 1, 1)]\n",
    "\n",
    "# parse_rulex(root_folder, bank_df, file_rules_name, conclusion_set, \"bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cars file configuration\n",
    "# root_folder = \"./data/Cars/4_conditionals/\"\n",
    "# bank_df = pd.read_csv(root_folder + \"cars_numeric.csv\")\n",
    "# file_rules_name = root_folder + 'Scoring_Rules_Cars.xlsx'\n",
    "# conclusion_set = [(\"unacc\", 0, 0), (\"acc\", 1, 1)]\n",
    "\n",
    "# parse_rulex(root_folder, bank_df, file_rules_name, conclusion_set, \"cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Myocardial file configuration\n",
    "# root_folder = \"./data/Myocardial/4_conditions/\"\n",
    "# bank_df = pd.read_csv(\"./data/Myocardial/myocardial_numeric.csv\")\n",
    "# file_rules_name = root_folder + '/Scoring_Rules_Myocardial.xlsx'\n",
    "# conclusion_set = [(\"no\", 0, 0), (\"yes\", 1, 1)]\n",
    "\n",
    "# parse_rulex(root_folder, bank_df, file_rules_name, conclusion_set, \"myocardial\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
