{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pycodestyle_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from feature import Feature, Level\n",
    "from rule import Rule\n",
    "import random  # This is used to generate random level names.\n",
    "random.seed(10)\n",
    "import copy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_levels(text):\n",
    "    # print(\"Value at ({}, {}):\".format(index, column), df.loc[index, column])\n",
    "    # 'feature in {level, level, level, ...}'\n",
    "    pattern = r\"([^\\*]+) in \\{([^\\}]+)\\}\"\n",
    "\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    if matches:\n",
    "        feature = matches[0][0].strip(\"' \")\n",
    "        levels = [level.strip('\" ') for level in matches[0][1].split(',')]\n",
    "        #   print(\"Feature:\", feature)\n",
    "        #   print(\"Levels:\", levels)\n",
    "    else:\n",
    "        return False, False\n",
    "\n",
    "    return feature, levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_min_max_range(text, df):\n",
    "\n",
    "    EPSILON = 0.001\n",
    "\n",
    "    # Inequality like: '(value (<=, <)) feature ((<=, <) value)'\n",
    "    # Left and right size are optional, but it must have at least one.\n",
    "    # Operators can be < or <=\n",
    "    pattern = r'(-?\\d+(?:\\.\\d+)?|\\w+)\\s*([<>]=?)\\s*(-?\\d+(?:\\.\\d+)?|\\w+)(?:\\s*([<>]=?)\\s*(-?\\d+(?:\\.\\d+)?))?'\n",
    "\n",
    "    match = re.match(pattern, text)\n",
    "\n",
    "    if not match:\n",
    "        return False, False, False\n",
    "\n",
    "    groups = match.groups()\n",
    "    \n",
    "    # Using any() to check if any group is None\n",
    "    # Expression does do have 5 elements. Can be value (<, <=) word, or word (<, <=) value\n",
    "    if any(group is None for group in groups):\n",
    "\n",
    "        try: # value (<, <=) word\n",
    "            number1 = float(groups[0])\n",
    "            op1 = groups[1]\n",
    "            feature = groups[2]\n",
    "            op2 = None\n",
    "            number2 = None\n",
    "        except ValueError:\n",
    "            \n",
    "            try: # word (<, <=) value\n",
    "                feature = groups[0]\n",
    "                op1 = groups[1]\n",
    "                number1 = float(groups[2])\n",
    "                op2 = None\n",
    "                number2 = None\n",
    "            except ValueError:\n",
    "                print (\"Expression not identified\")\n",
    "                raise TypeError\n",
    "\n",
    "\n",
    "    else: # value (<, <=) word (<, <=) value\n",
    "        number1 = float(match.group(1)) if match.group(1) else None\n",
    "        op1 = match.group(2) if match.group(2) else None\n",
    "        feature = match.group(3)\n",
    "        op2 = match.group(4) if match.group(4) else None\n",
    "        number2 = float(match.group(5)) if match.group(5) else None\n",
    "\n",
    "\n",
    "    if number1 is None and number2 is None:\n",
    "        print(\"Number1:\", number1)\n",
    "        print(\"op1:\", op1)\n",
    "        print(\"feature:\", feature)\n",
    "        print(\"op2:\", op2)\n",
    "        print(\"Number2:\", number2)\n",
    "        print(text)\n",
    "        print (\"Range pattern could not identify any limits\")\n",
    "        raise TypeError\n",
    "\n",
    "\n",
    "    # Limited on the left and right\n",
    "    # This case can only have signs < and <=\n",
    "    if number1 is not None and number2 is not None:\n",
    "\n",
    "        min = number1\n",
    "        max = number2\n",
    "\n",
    "        if \"=\" not in op1:\n",
    "            min += EPSILON\n",
    "        \n",
    "        if \"=\" not in op2:\n",
    "            max -= EPSILON\n",
    "\n",
    "    # Only limited on the right\n",
    "    if number1 is None and op1 is None:\n",
    "        if \">\" in op2:\n",
    "            max = df[feature].max()\n",
    "            min = number2\n",
    "            if \"=\" not in op2:\n",
    "                min += EPSILON\n",
    "            \n",
    "        \n",
    "        if \"<\" in op2:\n",
    "            min = df[feature].min()\n",
    "            max = number2\n",
    "            if \"=\" not in op2:\n",
    "                max -= EPSILON\n",
    "    \n",
    "    # Only limited on the left\n",
    "    if number2 is None and op2 is None:\n",
    "        if \">\" in op1:\n",
    "            min = df[feature].min()            \n",
    "            max = number1\n",
    "            if \"=\" not in op1:\n",
    "                max -= EPSILON\n",
    "              \n",
    "        if \"<\" in op1:\n",
    "            max = df[feature].max()            \n",
    "            min = number1\n",
    "            if \"=\" not in op1:\n",
    "                min += EPSILON\n",
    "\n",
    "    # print(\"First number:\", number1)\n",
    "    # print(\"Operator 1:\", op1)\n",
    "    # print(\"Word:\", feature)\n",
    "    # print(\"Operator 2:\", op2)\n",
    "    # print(\"Second number:\", number2)\n",
    "    \n",
    "    return feature, min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rules_and_update_feature_set(rules_df: pd.core.frame.DataFrame,  # DF with rules from Rulex\n",
    "                                        df: pd.core.frame.DataFrame,  # DF with data used to built rules\n",
    "                                        # Conclusion set as a list of tuples [(category, min max), ...]\n",
    "                                        conclusion_set: list,\n",
    "                                        categories_set: set,  # Set of possible category values to be employed\n",
    "                                        # Current list with all features\n",
    "                                        feature_set: list[Feature] = None) -> tuple:\n",
    "    # and levels that will be updated\n",
    "    \"\"\" Dataframe for rules needs a structure like this,\n",
    "            ID  Score Normalized   Condition 1      Condition 2             Condition 3            \n",
    "        0\t1\t0.25               0 < age <= 35\t'education-num' <= 12\t'capital-gain' <= 5119\n",
    "\n",
    "        Condition 4 \n",
    "        relationship in {\"Not-in-family\", \"Other-relative\", \"Own-child\", Unmarried}\n",
    "\n",
    "        Hence, the conditions for each rule can be an inequalitity or set of strings.\n",
    "\n",
    "        The original dataframe (df) is necessary to get min and max for unbounded ranges.\n",
    "\n",
    "        This Function will update the feature set list like: [Feature, Feature, Feature, ...]\n",
    "        Each instance of Feature contains a list of levels each of type Level\n",
    "\n",
    "        This function also returns a list of Rules extracted from the rules_df.\n",
    "        This list can later be imported by the argumentation framework.\n",
    "    \"\"\"\n",
    "    def append_feature(feature_list, feature) -> None:\n",
    "        for f in feature_list:  # Check if feature is already in the list\n",
    "            if f.name == feature:\n",
    "                break\n",
    "        else:\n",
    "            feature_list.append(Feature(feature))\n",
    "\n",
    "    if feature_set is None:\n",
    "        feature_set = []\n",
    "\n",
    "    # List of Rules to be created\n",
    "    rules = []  \n",
    "\n",
    "    for index in rules_df.index:\n",
    "        # Create current rule\n",
    "        rule = Rule(index)\n",
    "\n",
    "        for column in rules_df.columns:\n",
    "\n",
    "            if \"Score Normalized\" in column:\n",
    "                rule.weight = rules_df.loc[index, column]\n",
    "\n",
    "            # If it is not Condiction, skip\n",
    "            if \"Condition\" not in column:\n",
    "                continue\n",
    "            \n",
    "            # If rules does not have the condition, skip\n",
    "            if type(rules_df.loc[index, column]) != str:\n",
    "                continue\n",
    "\n",
    "            # Get feature and levels if condition is categorical\n",
    "            feature, levels = extract_feature_levels(rules_df.loc[index, column])\n",
    "\n",
    "            if feature:\n",
    "\n",
    "                append_feature(feature_set, feature)\n",
    "\n",
    "                # Need to find feature in the list\n",
    "                for i, f in enumerate(feature_set):\n",
    "\n",
    "                    if f.name != feature:\n",
    "                        continue\n",
    "\n",
    "                    for level in levels:\n",
    "\n",
    "                        # Debug condition and extracted feature level\n",
    "                        # print(rules_df.loc[index, column])\n",
    "\n",
    "                        try:\n",
    "                            # If the category is a number, there is no need to assign a counter.\n",
    "                            new_level = Level(\n",
    "                                feature, \"category\", level, float(level), float(level))\n",
    "                            # Level value employed is discarded from possible values to be used\n",
    "                            categories_set.discard(float(level))\n",
    "                            # Debug condition and extracted feature level\n",
    "                            # print(feature, level, float(level), float(level))\n",
    "                        except ValueError:\n",
    "                            # Get a possible value from the set of values not used to assign to this category\n",
    "                            value = next(iter(categories_set))\n",
    "                            new_level = Level(\n",
    "                                feature, \"category\", level, value, value)\n",
    "                            # Level value employed is discarded from possible values to be used\n",
    "                            categories_set.discard(value)\n",
    "                            # Debug condition and extracted feature level\n",
    "                            # print(feature, level, len(feature_set[i]) + 1, len(feature_set[i]) + 1)\n",
    "\n",
    "                        # Try to add in case there is not another level with the same range\n",
    "                        feature_set[i].add_level(new_level)\n",
    "                        #rule_conditions.append(copy.deepcopy(new_level))\n",
    "                        rule.conditions.append(copy.deepcopy(new_level))\n",
    "\n",
    "                    break  # Feature found in the list\n",
    "\n",
    "                continue  # Got to next condition\n",
    "\n",
    "            # Get min and max range if condition is categorical\n",
    "            feature, min, max = extract_min_max_range(rules_df.loc[index, column], df)\n",
    "\n",
    "\n",
    "            # print(\"f:\", feature)\n",
    "            # print(\"min\", min)\n",
    "            # print(\"max:\", max)\n",
    "\n",
    "            if feature:\n",
    "\n",
    "                # Check if feature is already in the list\n",
    "                append_feature(feature_set, feature)\n",
    "\n",
    "                # Need to find feature in the list\n",
    "                for i, f in enumerate(feature_set):\n",
    "\n",
    "                    if f.name != feature:\n",
    "                        continue\n",
    "\n",
    "                    # Debug condition and extracted feature level\n",
    "                    # print(rules_df.loc[index, column])\n",
    "                    # print(feature, min, max)\n",
    "\n",
    "                    new_level = Level(feature, \"range\",\n",
    "                                      Feature.get_new_level(), min, max)\n",
    "                    feature_set[i].add_level(new_level)\n",
    "\n",
    "                    # Get level is necessary in case we try to add a level that was already there\n",
    "                    # rule_conditions.append(copy.deepcopy(\n",
    "                    #     feature_set[i].get_level(min, max)))\n",
    "                    rule.conditions.append(copy.deepcopy(\n",
    "                        feature_set[i].get_level(min, max)))\n",
    "\n",
    "                continue  # Got to next condition\n",
    "\n",
    "            print(\"Condition does not match any pattern\")\n",
    "            print(rules_df.loc[index, column])\n",
    "            raise TypeError\n",
    "\n",
    "        rule.rule_str = rule.get_rule_str(rules_df, conclusion_set)\n",
    "        #print(rule.rule_str)\n",
    "        rules.append(rule)\n",
    "\n",
    "    return feature_set, rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_set_to_json(feature_set: list[Feature],\n",
    "                        conclusion_set: list[tuple],\n",
    "                        name: str = \"featureset\") -> str:\n",
    "    \"\"\" Get feature_set from create_feature_set and a conclusion set in a list of tuples.\n",
    "        Returns the json string to be imported as the feature_set\n",
    "        Example of json structure being employed: https://lucasrizzo.com/framework/json_example.png\n",
    "    \"\"\"\n",
    "\n",
    "    json_str = f\"{{\\\"featureset\\\":\\\"{name}\\\",\"\n",
    "    json_str += f\"\\n\\t\\\"attributes\\\":[\"\n",
    "\n",
    "    for att in feature_set:\n",
    "        json_str += f\"\\n\\t\\t[{{\\\"name\\\":\\\"{att.name}\\\"}},\"\n",
    "        json_str += f\"\\n\\t\\t {{\\\"range\\\":\\\"{len(att)}\\\"}},\"\n",
    "\n",
    "        # From loop\n",
    "        json_str += f\"\\n\\t\\t {{\\\"from\\\":[\"\n",
    "        for level in att.levels:\n",
    "            json_str += f\"\\n\\t\\t  \\t{{\\\"value\\\":\\\"{level.min:.4f}\\\"}},\"\n",
    "\n",
    "        json_str = json_str[:-1]  # Remove last coma\n",
    "        json_str += \"]},\"\n",
    "\n",
    "        # To loop\n",
    "        json_str += f\"\\n\\t\\t {{\\\"to\\\":[\"\n",
    "        # for level in feature_set[att]:\n",
    "        for level in att.levels:\n",
    "            json_str += f\"\\n\\t\\t  \\t{{\\\"value\\\":\\\"{level.max:.4f}\\\"}},\"\n",
    "\n",
    "        json_str = json_str[:-1]  # Remove last coma\n",
    "        json_str += \"]},\"\n",
    "\n",
    "        # Level loop\n",
    "        json_str += f\"\\n\\t\\t {{\\\"level\\\":[\"\n",
    "        for level in att.levels:\n",
    "            json_str += f\"\\n\\t\\t  \\t{{\\\"value\\\":\\\"{level.name}\\\"}},\"\n",
    "\n",
    "        json_str = json_str[:-1]  # Remove last coma\n",
    "        json_str += \"]}],\"\n",
    "\n",
    "    json_str = json_str[:-1]  # Remove last coma\n",
    "    json_str += \"\\n\\t],\"\n",
    "\n",
    "    json_str += f\"\\n\\t\\\"conclusions\\\":[\"\n",
    "\n",
    "    # for key, value in conclusion_set.items():\n",
    "    for name, min, max in conclusion_set:\n",
    "        json_str += f\"\\n\\t\\t[{{\\\"category\\\":\\\"{name}\\\"}},\"\n",
    "        json_str += f\"\\n\\t\\t {{\\\"from\\\":\\\"{min}\\\"}},\"\n",
    "        json_str += f\"\\n\\t\\t {{\\\"to\\\":\\\"{max}\\\"}}],\"\n",
    "\n",
    "    json_str = json_str[:-1] + \"]\"  # Remove last coma\n",
    "    json_str += \"\\n}\"\n",
    "\n",
    "    return json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules_to_json(rules: list[Rule]) -> None:\n",
    "\n",
    "    json_result = '{\"nodes\":['\n",
    "    node_id = 0\n",
    "    x = 0\n",
    "    y = 0\n",
    "\n",
    "    for rule in rules:\n",
    "        json_result += f'{{\\\"id\\\":{node_id},'\n",
    "        json_result += f'\\\"title\\\":\\\"R{node_id + 1}\\\",'\n",
    "        json_result += f'\\\"x\\\":{x},'\n",
    "        json_result += f'\\\"y\\\":{y},'\n",
    "        json_result += f'\\\"weight\\\":\"{rule.weight}\",'\n",
    "        json_result += f'\\\"tooltip\\\":\\\"{\n",
    "            rule.rule_str.replace('\"', '\\\\\"').strip(\"\\n\")}\\\"}},'\n",
    "        x += 150\n",
    "        node_id += 1\n",
    "        if x == 1050:  # After 6 nodes, restart row\n",
    "            x = 0\n",
    "            y += 150\n",
    "\n",
    "    json_result = json_result[:-1] + \"],\\\"edges\\\":[]}\"\n",
    "    return json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rulex(root_folder: str,  # root folder where files are being stored\n",
    "                df: pd.core.frame.DataFrame,  # data frame used to generate the rules\n",
    "                file_rules_name: str,  # xlsx file with all the rules generated by rules\n",
    "                conclusion_set: list[tuple],  # Conclusion set of rules\n",
    "                data_name: str,  # Name used to the feature set\n",
    "                max_categories: int = 1000000) -> None:\n",
    "\n",
    "    random_values = set()\n",
    "    while len(random_values) < max_categories:\n",
    "        random_values.add(random.randint(0, max_categories * 10))\n",
    "\n",
    "    xls = pd.ExcelFile(file_rules_name)\n",
    "\n",
    "    feature_set = []\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        rules_df = pd.read_excel(file_rules_name, sheet_name=sheet_name)\n",
    "\n",
    "        file_rule_name = \"\".join(x for x in sheet_name if x.isalnum()) + \"_rules\"\n",
    "        rule_file_txt = root_folder + \"results_framework/\" + file_rule_name + \".txt\"\n",
    "        rule_file_json = root_folder + \"results_framework/\" + file_rule_name + \".json\"\n",
    "\n",
    "        feature_set, rules = create_rules_and_update_feature_set(rules_df, df, conclusion_set, random_values, feature_set)\n",
    "\n",
    "        file = open(rule_file_txt, \"w\")\n",
    "        for rule in rules:\n",
    "            file.write(str(rule))\n",
    "            file.write(\"\\n\")\n",
    "        file.close()\n",
    "\n",
    "        file = open(rule_file_json, \"w\")\n",
    "        file.write(rules_to_json(rules))\n",
    "        file.close()\n",
    "\n",
    "    json_str = \"\"\n",
    "    json_str += feature_set_to_json(feature_set, conclusion_set, data_name)\n",
    "    file = open(root_folder + \"results_framework/\" + data_name.capitalize() + \"_featureset.json\", \"w\")\n",
    "    file.write(json_str)\n",
    "    file.close()\n",
    "    # print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cirrhosis file configuration\n",
    "# root_folder = \"./Cirrhosis/\"\n",
    "# bank_df = pd.read_csv(\"./Cirrhosis/cirrhosis_numeric.csv\")\n",
    "# file_rules_name = root_folder + '/rulex_results/Scoring_Rules_Cirrhosis.xlsx'\n",
    "# conclusion_set = [(1, 1, 1), (2, 2, 2), (3, 3, 3), (4, 4, 4)]\n",
    "\n",
    "# parse_rulex(root_folder, bank_df, file_rules_name, conclusion_set, \"cirrhosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic file configuration\n",
    "root_folder = \"./Genetic/\"\n",
    "bank_df = pd.read_csv(\"./Genetic/GDS4968_numeric.csv\")\n",
    "file_rules_name = root_folder + '/rulex_results/Scoring_Rules_genetic_multiclass.xlsx'\n",
    "conclusion_set = [(0, 0, 0), (1, 1, 1), (2, 2, 2), (3, 3, 3)]\n",
    "\n",
    "parse_rulex(root_folder, bank_df, file_rules_name, conclusion_set, \"cirrhosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Soccer file configuration\n",
    "# root_folder = \"./Soccer/\"\n",
    "# #bank_df = pd.read_csv(\"./Soccer/soccer_input.csv\")\n",
    "# file_rules_name = root_folder + '/rulex_results/Scoring_Rules_Soccer.xlsx'\n",
    "# conclusion_set = [(1, 1, 1), (2, 2, 2), (\"X\", 3, 3)]\n",
    "\n",
    "# #parse_rulex(root_folder, bank_df, file_rules_name, conclusion_set, \"soccer\")\n",
    "# parse_rulex(root_folder, \"\", file_rules_name, conclusion_set, \"soccer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cars file configuration\n",
    "# root_folder = \"./Cars/\"\n",
    "# bank_df = pd.read_csv(\"./Cars/cars_numeric.csv\")\n",
    "# file_rules_name = root_folder + 'rulex_results/Scoring_Rules_Cars_multi.xlsx'\n",
    "# conclusion_set = [(\"unacc\", 0, 0), (\"acc\", 1, 1), (\"good\", 2, 2), (\"vgood\", 3, 3)]\n",
    "\n",
    "# parse_rulex(root_folder, bank_df, file_rules_name, conclusion_set, \"cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Myocardial file configuration\n",
    "# root_folder = \"./data/Myocardial/4_conditions/\"\n",
    "# bank_df = pd.read_csv(\"./data/Myocardial/myocardial_numeric.csv\")\n",
    "# file_rules_name = root_folder + '/Scoring_Rules_Myocardial.xlsx'\n",
    "# conclusion_set = [(\"no\", 0, 0), (\"yes\", 1, 1)]\n",
    "\n",
    "# parse_rulex(root_folder, bank_df, file_rules_name, conclusion_set, \"myocardial\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
